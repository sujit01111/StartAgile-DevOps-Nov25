Container Orchestration with Kubernetes
Kubernetes and its features
Compare Docker swarm and Kubernetes
Installation of Kubernetes on EC2 server - Kubernetes cluster
What is a POD in Kubernetes

Kubernetes: - K8s
1. It is an open source tool
2. It was a product of google and is build using go language
3. Kubernetes is not like docker , you cannot create images in Kubernetes - It is not a container runtime engine (docker)
4. Kubernetes is the next step after docker -> It is container orchestration tool
5. Kubernetes provides  features like:
  - creating n number of containers from 1 image - Replicas
  - Distribute container on many VMs
  - Auto Scaling of containers 
  - High availability of containers
  - rolling update deployment strategy
6. Kubernetes is a production grade orchestration tool
7. Support auto scaling of containers based on the actual user traffic
8. Supports storing of the container data in external storage drivers - NFS, EBS, PD, Azure disk
9. It also is present on all the 3 clouds as a service - AKS, EKS, GKE - readymade kubernetes clusters 
10.In Kubernetes we have jobs and Crons job to schedule creation of containers at a given time 
11. Kubernetes is a production grade tool it supports network policies for secure access to your containers 
12. Kubernetes is a production grade it supports implementation of ingress - network load balancing
13. Kubernetes is a production grade supports RBAC and cluster wide role access


Docker Swarm 
"Doesnot require to be installed
Comes pre installed with Docker"
Exclusive orchestration tool for docker
No Autoscaling
No storage of container data in external storage
comes with inbuot network - docker 0 network
It provides only an object called service
Command can be run on docker swarm machine and container are created on the same machine
Free version doesnot have any GUI
No such feature
No such feature
No such feature


Kubernetes
We have to install kubernetes manster and worker node
It is a global tool, it can orchestrate containers of any container runtime like CRI-O, contaienrd, docker, RKT containers
Horizontal and vertical auto scaling
external storage association for contaienr data
We have to install container network plugin
"here for every orchestration task there is separate object:
container: Pod
replicas : replicaset object
rolling update: deployment object
port mapping: services
autoscaling: HPA, VPA objects"
Commands by default will be run on master machine and contaienr will be created on worker machines
Has a GUI dashboard
Can be integrated with monitoring tools like prometheus and grafana
Comes with helm for automation
can be integrated with gitops tools


orchestration: 
creating many container from an image 
scale up and scale down of containers 
ensure desired count of container is running

Kubernetes Master 
- all Kubernetes orchestration commands will run on Master 
- Assume it to be like a leader in a team 
- 4 components will be installed automatically [apiServer, schedular, etcd, controller manager]
- On master you execute the command to create container 

Kubernetes Worker
- no orchestration commands will run on Worker 
- Assume it to be like a team member 
- 2 components to be installed [ kubeproxy, kubelet]
- container will be deployed and run on the worker 
- a worker has to connect to the master



POD in kubernetes:
=====================================================
In docker if we run an image we get a container 

# docker run nginx -> get a container with nginx app on it 

In kubernetes when we run an image we get a pod 

A pod consist of a container of the image that we have run

# kubectl run pod1 --image nginx

-> kubernetes run the image nginx, create apod with name pod1 and in the pod is a container with application nginx

WHY POD??

1 main feature of kubernetes is to ensure when an image is run -> container is created -> contaienr is always running
if the app on the container is stopped--> immediately it should be restarted --> who will do this???
kubernetes gives us POD to do this

WHAT is a pod:
POD is a simple and smallest object in kubernetes that we can deploy 
A pod is like a box which holds your container 
A pod will be responsible to restart container in case it fails. we do not have to worry.
So in Kubernetes we say-> we create a pod --> which is nothing but creating a container from an Image

Installation of Kubernetes:
==========================================================

Steps on MASTER NODE:
============================================
# sudo su -

## Install Containerd

sudo wget https://raw.githubusercontent.com/lerndevops/labs/master/scripts/installContainerd.sh -P /tmp
sudo bash /tmp/installContainerd.sh
sudo systemctl restart containerd.service


### Install kubeadm,kubelet,kubectl

You will install these packages on all of your machines:
kubeadm: the command to bootstrap the cluster.
kubelet: the component that runs on all of the machines in your cluster and does things like starting pods and containers.
kubectl: the command line util to talk to your cluster.


sudo wget https://raw.githubusercontent.com/lerndevops/labs/master/scripts/installK8S.sh -P /tmp

sudo bash /tmp/installK8S.sh

## Initialize kubernetes Master Node

   # sudo kubeadm init --ignore-preflight-errors=all

Execute the below commands to setup kubectl and apiserver communication

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config


   ## install networking driver -- Weave/flannel/canal/calico etc...

   ## below installs calico networking driver

   kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.24.1/manifests/calico.yaml

   # Validate:  kubectl get nodes



========================================

On All Worker Nodes


## Install Containerd
# sudo su -
sudo wget https://raw.githubusercontent.com/lerndevops/labs/master/scripts/installContainerd.sh -P /tmp
sudo bash /tmp/installContainerd.sh
sudo systemctl restart containerd.service

## Install kubeadm,kubelet,kubectl

sudo wget https://raw.githubusercontent.com/lerndevops/labs/master/scripts/installK8S.sh -P /tmp
sudo bash /tmp/installK8S.sh

Step 2: Install kubeadm, kubelet on Worker Node
sudo apt-get update
# apt-transport-https may be a dummy package; if so, you can skip that package
sudo apt-get install -y apt-transport-https ca-certificates curl gpg|
curl -fsSL https://pkgs.k8s.io/core/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o
/etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg]
https://pkgs.k8s.io/core:/stable:/v1.30/deb/ / | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt-get update
sudo apt-get install -y kubelet kubeadm
sudo apt-mark hold kubelet kubeadm
Step 3: Copy the token and worker joins master

## Run Below on Master Node to get join token

#  kubeadm token create --print-join-command

    copy the kubeadm join token from master & run it on all worker nodes

On Master node:

# kubectl get nodes
==================================
Demo on PODs:
===================================

# kubectl run pod1  --image nginx

# kubectl get pods

# kubectl get pods -o wide

# kubectl describe pod pod1 | less

=============================================================
Agenda: 27 Nov:
=============================================================

1. Types of pod
    - Single container Pod
    - Multi  Container Pod
2. Creating multiple Pods - ReplicaSet
3. Access the pod from browser-> Service 



Demo 1 : Multi container POD:
=================================

apiVersion: v1
kind: Pod
metadata: 
  name: multi-pod-1 # this is pod name
  labels:  # used to group same pods and then can be used for querying in kubernetes
    author: sonal
    app: java
    type: webserver
spec:
  containers:
    - name: c1  # this is container name 
      image: nginx
    - name: c2
      image: httpd
    - name: c3
      image: ubuntu

Agenda: 28 Nov 2025

-> ReplicaSet
     - replicas 
     - ensures desired count of replias = current count
     - scale up and scale down
-> Deployment
    -> ReplicaSet
     - replicas 
     - ensures desired count of replias = current count
     - scale up and scale down
    -> Changing Image on the Deployment

ReplicaSet:
===================
# ReplicaSet : create a set of Replicas(pods)
# ReplicaSet : It is a loop running in the cluster-
# ensure your desired count of replicas are always running
# example: I desire 5 replicas
# we have to create replicaset->on this mention how many replica->5->
#  give us 5 replicas(pods)-> current = desired pods
# now i desire 10 pods --> current 5, desired 10 - mismatch
# immediatly replicaset will create 5 more new pods -> current 10, desired 10
# we create replicasset, which in turn creates a pod

# how to create a replicaset -> we write below YAML

---
kind: ReplicaSet 
apiVersion: apps/v1
metadata: 
   name: myrs
spec:
   selector:   # will give current count
     matchLabels:
       type: webserver
   replicas: 4 # how many pods to be created  - desired count
   template: # this is pod template, using this the 4 pods will be created
      metadata:
        labels:
          type: webserver
      spec:
        containers:
           - name: c1
             image: nginx 


Deployment:
========================================

Recreate Deployment Strategy


  The Recreate deployment strategy in Kubernetes is a method where all existing Pods are terminated before new ones are created during a deployment update.

  This strategy is useful when:

  The application doesnâ€™t support multiple versions running simultaneously.

  There are conflicts with shared resources (e.g., using the same persistent storage).

  You want zero overlap between old and new versions.



# vim deployment-recrete.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx
        ports:
        - containerPort: 80


Save and run the yaml

Now run the command to update the image and see all 3 pods will be terminated and new pods will be created.
# kubectl set image deployment/nginx-deployment nginx=nginx:1.26

# kubectl get all

# kubectl rollout status deployment <deploymentname>

Go back to previous version:

# kubectl rollout undo deployment <deploymentName>

========================================
MatchExpressions in Deployment or ReplicaSet YAML
========================================
4 types of operators: In, NotIn, Exist, DoesNotExist


apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchExpressions:
      - key: app
        operator: In
        values:
          - nginx
          - webserver
      - key: env
        operator: NotIn
        values:
          - prod
  template:
    metadata:
      name: nginxpod
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:latest
          resources:
            limits:
              cpu: 10m
=======================================
Rolling Update Deployment

# vim deployment.yml

kind: Deployment
apiVersion: apps/v1
metadata:
  name: kubeserve
spec:
  replicas: 3
  minReadySeconds: 10 # wait for 45 sec before going to deploy next pod
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1  
      maxSurge: 1        # max number of pods to run for the deployment
  selector:
    matchLabels:
      app: kubeserve
  template:
    metadata:
      name: kubeserve
      labels:
        app: kubeserve
    spec:
      containers:
       - name: app
         image: leaddevops/kubeserve:v1
        


# kubectl create -f deployment.yml 
# kubectl get deployment
# kubectl get all
# kubectl scale deployment kubeserve --replicas=5
# kubectl get pods
# kubectl scale deployment kubeserve --replicas=2
Change the Image
# kubectl set image deployment kubeserve app=leaddevops/kubeserve:v2
# kubectl rollout status
# kubectl set image deployment kubeserve app=leaddevops/kubeserve:v3
# kubectl rollout status deployment kubeserve
V3 is faulty.. so rollback to previous version
# kubectl rollout undo deployment kubeserve










HPA:
===============================

Horizontal Pod Autoscaler
========================================================

Instal Metric Server:

kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

wget -c  https://gist.githubusercontent.com/initcron/1a2bd25353e1faa22a0ad41ad1c01b62/raw/008e23f9fbf4d7e2cf79df1dd008de2f1db62a10/k8s-metrics-server.patch.yaml

kubectl patch deploy metrics-server -p "$(cat k8s-metrics-server.patch.yaml)" -n kube-system



# kubectl get pods -n kube-system

# cd

# cd mykubefiles

# vim hpa.yml

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      name: nginxpod
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:latest
          resources:
            limits:
              cpu: 10m

---

apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
spec:
  type: ClusterIP  ## this is default if we do not type in service definition
  selector:
    app: nginx
  ports:
   - protocol: TCP
     port: 80
     targetPort: 80

---

apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 5



Save the file

# kubectl delete all --all

# kubectl create -f hpa.yml

# kubectl get all

# Now launch the load generator pod -> open master terminal again

# kubectl run -i --tty load-generator --rm --image=busybox --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://Service-Internal-IP:80; done"

Check the pods:

kubectl get pods
kubectl top pods

Explanation for HPA: 
=================
A Deployment can scale up and scale down the pods -->

but if deployment has to scale the pods automatically as the number of user request are increasing.. how will it know about that??

Metric Server --> computes and displays the CPU and memory of each pod in the cluster 

Deployment needs a friend that can inform it that CPU of the pods has been utilized now scale up the pods

That friend is HPA 

HPA uses metric server to monitor the CPU of  the pods that are associated with the deployment that we mentioned in YAML 
scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx

HPA will able to tell you current CPU usage of the all Pods in the deployment

HPA uses this formula to compute the desired number of replicas that deployment should maintain 

Desired Count = Current replicas * (current utilization)/target utilization

targetCPUUtilizationPercentage: 50 

Current replicas = 4 pods 

current utilization = average CPU utilization of all the pods associated currently with the deployment 

current utilization = 300 milli cpu core /pod

CPU utilization of deployment  that HPA will compute 

CPU utilization = 300/50 * 100 ==> 60 %

Based on this  how HAP will compute the desired count of replicas for the deployment 

Desired Count = Current replicas * (current utilization)/target utilization


Desired Count = 4 * 60/50 = 4.8 ==> rounded of 5 

Desired Count = 5

HPA will tell deployment to scale --replicas = 5







